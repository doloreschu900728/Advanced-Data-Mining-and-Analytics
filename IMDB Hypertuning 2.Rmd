---
title: "IMDB Hypertuning 2"
author: "Dolores Chu"
date: "2/9/2020"
output: html_document
---

```{r}
library (keras)
install_keras()
imdb <- dataset_imdb(num_words = 10000)
set.seed(123)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb

vectorize_sequences <- function(sequences, dimension = 10000) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results
}
x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)

y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```


Orignal Model
```{r}
original_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

original_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

original_model_hist <- original_model %>% fit(x_train, y_train, epochs = 20, batch_size = 512, validation_data = list(x_test, y_test))
```


Use any technique we studied in class, and these include regularization, dropout, etc., to get your model to perform better on validation.
Upload the following to your github account. 

Adding regularization
```{r}
model_l2 <- keras_model_sequential() %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001), activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16,  kernel_regularizer = regularizer_l2(0.001), activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model_l2 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

model_l2_hist <- model_l2 %>% fit(x_train, y_train, epochs = 20, batch_size = 512, validation_data = list(x_test, y_test))
```

Plotting comparison between the original model and the regularized model
```{r}
library(ggplot2)
library(tidyr)
plot_training_losses <- function(losses) {
  loss_names <- names(losses)
  losses <- as.data.frame(losses)
  losses$epoch <- seq_len(nrow(losses))
  losses %>% 
    gather(model, loss, loss_names[[1]], loss_names[[2]]) %>% 
    ggplot(aes(x = epoch, y = loss, colour = model)) +
    geom_point()
}
```

```{r}
plot_training_losses(losses = list(original_model = original_model_hist$metrics$val_loss,
  model_l2 = model_l2_hist$metrics$val_loss
))
```



Add drop out.
```{r}
model_dpt <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")

model_dpt %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

model_dpt_hist<- model_dpt %>% fit(x_train, y_train, epochs = 20, batch_size = 512, validation_data = list(x_test, y_test))
```

Compare original model and drop-out model.
```{r}
plot_training_losses(losses = list(original_model = original_model_hist$metrics$val_loss,
  model_dpt = model_dpt_hist$metrics$val_loss
))
```